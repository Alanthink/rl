<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'] ],
    processEscapes: true
  },
  TeX: {
    equationNumbers: {
      autoNumber: "AMS"
    },
    Macros: {
      eps: "{\\epsilon}",
      E: "{\\mathbb{E}}",
      Pr: "{\\mathrm{Pr}}",
      cond: "{~|~}",
      br: "{\\mathrm{BR}}",
      eqdef: "{\\,\\mathbin{\\stackrel{\\rm def}{=}} \\,}"
    },
    extensions: ["AMSmath.js", "AMSsymbols.js"]
  }
});
</script>
<link rel="shortcut icon" href="school.ico" type="image/x-icon">
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/latest.js?config=TeX-AMS_SVG">
</script>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Reading Group @ SICE, IUB</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Reading Group @ SICE, IUB</h1>
</div>
<p>Welcome to the homepage of our reading group for reinforcement learning.</p>
<h2>Info</h2>
<p>Host: <a href="https://alanthink.github.io/">Chao Tao</a></p>
<p>Time: bi-weekly on 1:00pm - 3:00pm</p>
<p>Place: Lindley Hall 3069</p>
<h2>Schedule</h2>
<ul>
<li><p>1/17, <a href="https://alanthink.github.io/">Chao Tao</a> on Introduction to Reinforcement Learning</p>
</li>
</ul>
<ul>
<div class="infoblock">
<div class="blocktitle">Summary</div>
<div class="blockcontent">
<p>TBD</p>
</div></div>
</ul>
<ul>
<li><p>1/31, <a href="https://alanthink.github.io/">Chao Tao</a> on <a href="https://arxiv.org/abs/1703.05449">Minimax regret bounds for reinforcement learning</a></p>
</li>
</ul>
<ul>
<li><p>2/14, <a href="https://alanthink.github.io/">Chao Tao</a> on <a href="https://papers.nips.cc/paper/7735-is-q-learning-provably-efficient">Is Q-learning Provably Efficient?</a></p>
</li>
</ul>
<ul>
<li><p>2/28, <a href="https://alanthink.github.io/">Chao Tao</a> on <a href="https://arxiv.org/abs/1709.04570">Learning Unknown Markov Decision Processes: A Thompson Sampling Approach</a></p>
</li>
</ul>
<ul>
<li><p>3/13, <a href="https://alanthink.github.io/">Chao Tao</a> on <a href="http://proceedings.mlr.press/v70/osband17a.html">Why is Posterior Sampling Better than Optimism for Reinforcement Learning?</a></p>
</li>
</ul>
<ul>
<li><p>3/27, TBD</p>
</li>
</ul>
<ul>
<li><p>4/10, TBD</p>
</li>
</ul>
<ul>
<li><p>4/24, TBD</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
