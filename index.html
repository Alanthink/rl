<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'] ],
    processEscapes: true
  },
  TeX: {
    equationNumbers: {
      autoNumber: "AMS"
    },
    Macros: {
      eps: "{\\epsilon}",
      E: "{\\mathbb{E}}",
      Pr: "{\\mathrm{Pr}}",
      cond: "{~|~}",
      br: "{\\mathrm{BR}}",
      eqdef: "{\\,\\mathbin{\\stackrel{\\rm def}{=}} \\,}"
    },
    extensions: ["AMSmath.js", "AMSsymbols.js"]
  }
});
</script>
<link rel="shortcut icon" href="school.ico" type="image/x-icon">
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/latest.js?config=TeX-AMS_SVG">
</script>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Reading Group @ SICE, IUB</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Reading Group @ SICE, IUB</h1>
</div>
<p>Welcome to the homepage of our reading group for reinforcement learning.</p>
<h2>Info</h2>
<p>Host: <a href="https://alanthink.github.io/">Chao Tao</a></p>
<p>Time: bi-weekly on 1:00pm - 3:00pm</p>
<p>Place: Luddy Hall 3069</p>
<h2>Schedule</h2>
<ul>
<li><p>1/17, <a href="https://alanthink.github.io/">Chao Tao</a> on Introduction to Reinforcement Learning (<a href="lec1.pdf">notes</a>)</p>
</li>
</ul>
<ul>
<div class="infoblock">
<div class="blocktitle">Keywords</div>
<div class="blockcontent">
<p>Markov Decision Process (MDP); Value Functions; Bellman Equations; State Occupancy; Q Learning;</p>
</div></div>
</ul>
<ul>
<li><p>1/31, <a href="https://alanthink.github.io/">Chao Tao</a> on <a href="https://arxiv.org/abs/1703.05449">Minimax regret bounds for reinforcement learning</a> (<a href="lec2.pdf">notes</a>)</p>
</li>
</ul>
<ul>
<div class="infoblock">
<div class="blocktitle">Keywords</div>
<div class="blockcontent">
<p>Tabular Episodic MDP; Model-based; Optimistic Algorithm; Frequentist Regret; $\widetilde{\mathcal{O}}(H\sqrt{SAT})$;</p>
</div></div>
</ul>
<ul>
<li><p>2/14, <a href="https://alanthink.github.io/">Chao Tao</a> on <a href="https://arxiv.org/abs/1807.03765">Is Q-learning Provably Efficient?</a> (<a href="lec3.pdf">notes</a>)</p>
</li>
</ul>
<ul>
<div class="infoblock">
<div class="blocktitle">Keywords</div>
<div class="blockcontent">
<p>Tabular Episodic MDP; Model-free; Optimistic Algorithm; Frequentist Regret; $\widetilde{\mathcal{O}}(H^2\sqrt{SAT})$;</p>
</div></div>
</ul>
<ul>
<li><p>2/28, <a href="https://alanthink.github.io/">Chao Tao</a> on <a href="https://arxiv.org/abs/1306.0940">(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
</li>
</ul>
<ul>
<div class="infoblock">
<div class="blocktitle">Keywords</div>
<div class="blockcontent">
<p>Tabular Episodic MDP; Thompson Sampling; Bayesian Regret; $\widetilde{\mathcal{O}}(HS\sqrt{AT})$;</p>
</div></div>
</ul>
<ul>
<li><p>3/13, <a href="https://alanthink.github.io/">Chao Tao</a> on <a href="https://arxiv.org/abs/1607.00215">Why is Posterior Sampling Better than Optimism for Reinforcement Learning?</a></p>
</li>
</ul>
<ul>
<div class="infoblock">
<div class="blocktitle">Keywords</div>
<div class="blockcontent">
<p>Tabular Episodic MDP; Thompson Sampling; Bayesian Regret; $\widetilde{\mathcal{O}}(H\sqrt{SAT})$;</p>
</div></div>
</ul>
<ul>
<li><p>3/27, TBD</p>
</li>
</ul>
<ul>
<li><p>4/10, TBD</p>
</li>
</ul>
<ul>
<li><p>4/24, TBD</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated by 2020-02-11 17:25:07 EST, <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
